<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cerulean.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<div class="container-fluid main-container">

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->





<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Data Science</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="Data_Science.html">Data Science</a>
</li>
<li>
  <a href="All_topics.html">All Topics</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<div id="applying-the-central-limit-theorem" class="section level1">
<h1>Applying the Central Limit Theorem</h1>
<div id="an-example" class="section level2">
<h2>An example</h2>
<p>According the National Center for Health Statistics, the distribution of serum cholesterol levels for 20 to 74-year-old males living in the United States has mean 211 mg/dl, and a standard deviation of 46 mg/dl. We are planning to collect a sample of 25 individuals and measure their cholesterol levels.</p>
<p>We are interested in the following about the <em>sample</em>:</p>
<ol style="list-style-type: decimal">
<li>What is the probability that our <em>sample mean</em> will be above a certain limit, say 230?</li>
<li>What is the 95% confidence interval of our sample means?</li>
<li>How do these vary as we collect more samples? Does the probability increase or decrease? Does the size of the confidence interval increase or decrease? By what factor does it increase or decrease?</li>
<li>Finally, how large would the sample size have to be to ensure a 95% probability that the sample average is within 5 mg/dl of the population mean?</li>
</ol>
<p><em>How does the Central Limit Theorem (CLT) help us answer these questions?</em></p>
<hr />
</div>
<div id="central-limit-theorem" class="section level2">
<h2>Central Limit Theorem</h2>
<p>Given a population with a finite mean <span class="math inline">\(\mu\)</span> and a finite non-zero standard deviation <span class="math inline">\(\sigma\)</span>, the distribution of the sample means approaches a normal distribution as the sample size increases.</p>
<p>The mean of the sample means is given by</p>
<p><span class="math display">\[\mu_{\bar{X}} = \mu,\]</span></p>
<p>and the standard deviation of the sample means (also referred to as the <em>standard error of the mean</em>) is given by</p>
<p><span class="math display">\[\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{N}}.\]</span></p>
<p>For a more precise version of CLT, please refer <a href="http://mathworld.wolfram.com/CentralLimitTheorem.html">Wolfram</a>.</p>
<p>An important observation is that no assumptions are made about the distribution of the parental population. It could be discrete or continuous, severely skewed, but as long as the mean and standard deviation are finite, CLT holds. To convince yourself of this, please take a look at examples <a href="http://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/BS704_Probability/BS704_Probability12.html">here</a> or use the simulator <a href="http://onlinestatbook.com/stat_sim/sampling_dist/index.html">here</a>.</p>
<hr />
</div>
<div id="samples" class="section level2">
<h2>Samples</h2>
<p>But what is a <strong>sample</strong>? We keep using that word, and its meaning is quite an important concept. <span style="color:magenta">A sample is a random draw of size N of data from the parent distribution.</span> We obtain a sample of data every time we randomly draw from what we conceptualize as the population of interest. If the population of interest in every student at UMD, then a random draw is obtained by any mechanism that (truly) randomly draws from it (think of an imaginary lottery machine that, after spinning it, we can obtain one student at a time).</p>
<p>And the <strong>sample mean</strong>? The sample mean is just the average of the measure of interest from the <span class="math inline">\(N\)</span> units that were sampled. So for each mean, we get exactly one sample mean.</p>
<p>When we’re thinking about the CLT (what it means), we need to think of repeating this process many times to have <u>multiple sample means</u>. Remember, that each sample has <span class="math inline">\(N\)</span> units. So for each mean, you need to sample a group of size <span class="math inline">\(N\)</span>. This is what the CLT talks about.</p>
<p>One reason this might appear confusing is that in any one given study, we only sample once (with size <span class="math inline">\(N\)</span>). That’s the world the experimenter lives in (except it she repeats her experiment). But that’s not the world of the CLT, which instead is a world in which we perform an experiment (a sample draw of size <span class="math inline">\(N\)</span>), over and over. And a few more times…</p>
<hr />
<p>Back to the previous questions. To answer them, it is essential to know the <em>sampling distribution</em>, that is, the <em>distribution</em> of the <u>sample mean</u>.</p>
<ul>
<li>Since the standard deviation of the parent population is known, from CLT it follows that the sampling distribution (<span class="math inline">\(N=25\)</span>) has a mean <span class="math inline">\(\mu_{\bar{X}} = 211\)</span> mg/dl, and standard deviation (this is called <strong>standard error</strong>) <span class="math inline">\(\sigma_{\bar{X}} = \frac{46}{\sqrt{25}} = 9.2.\)</span> Note that the standard error reduces as the number of samples increase by a factor <span class="math inline">\(\sqrt{N}.\)</span></li>
<li>Our limit, <span class="math inline">\(230\)</span> mg/dl, is therefore <span class="math inline">\(\frac{230 - 211}{9.2} = 2.07\)</span> standard deviations away from the mean. In other words, the z-score associated with the limit <span class="math inline">\(230\)</span> mg/dl is <span class="math inline">\(2.07\)</span>.</li>
</ul>
<ol style="list-style-type: decimal">
<li>The answer to our first question is simple the probability that a normally distributed random variable is greater than <span class="math inline">\(2.07\)</span> standard deviations away from the mean = <span class="math inline">\(1.9\%\)</span> (or <span class="math inline">\(0.019\)</span>).</li>
<li>For a normally distributed random variable, 95% of the values lie within 1.96 standard deviations of its mean (on either side). The standard deviation remains the same, <span class="math inline">\(9.2\)</span>. Thus, the <span class="math inline">\(95\%\)</span> confidence interval is simply, 211 - <span class="math inline">\(1.96(9.2) = 193.0\)</span> to <span class="math inline">\(211 + 1.96(9.2) = 229.0\)</span>.</li>
<li>Suppose we had only <span class="math inline">\(10\)</span> samples. Verify that the new standard error would be <span class="math inline">\(14.5\)</span> and the z-score associated with the limit, <span class="math inline">\(230\)</span> mg/dl, would be <span class="math inline">\(1.31\)</span>. The probability of our sample mean being over <span class="math inline">\(230\)</span> would thus be <span class="math inline">\(9.6\%\)</span> (almost <span class="math inline">\(5\)</span> times higher). On the other hand, our confidence interval would be much larger with <span class="math inline">\(10\)</span> samples; <span class="math inline">\((182.5, 239.5)\)</span>. How about if we had <span class="math inline">\(50\)</span> samples? This would result is a narrower confidence interval <span class="math inline">\((198.2, 223.8)\)</span> since the standard error is smaller.</li>
<li>To answer our final question, we need <span class="math inline">\(1.96\)</span> standard deviations of the sampling distribution to amount to <span class="math inline">\(5\)</span> mg/dl. Thus, the standard error should be <span class="math inline">\(\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{N}} = 5/1.96.\)</span> Since, we know <span class="math inline">\(\sigma\)</span>, <span class="math inline">\(N = 325.1\)</span>. We would need at least <span class="math inline">\(326\)</span> samples to ensure this confidence interval.</li>
</ol>
<hr />
</div>
<div id="hypothesis-testing" class="section level2">
<h2>Hypothesis Testing</h2>
<p>Thinking along these lines can be used to develop hypothesis tests and understand p-values. We start again with an example:</p>
<p>Cystic fibrosis is a genetic disease that affects lung function. Forced vital capacity (FVC) is the volume of air that a person can expel from the lungs in <span class="math inline">\(6\)</span> seconds. It is often used as a marker of the progression of cystic fibrosis. <span class="math inline">\(14\)</span> participants received both a drug and placebo (at different times), and their FVC was measured at the beginning and end of each treatment period. In the study, the mean difference in reduction in FVC (placebo - drug) was <span class="math inline">\(137\)</span>, with a <u>sample standard deviation</u> <span class="math inline">\(223\)</span>. Does the drug have a <em>significant</em> effect?</p>
<p>The null hypothesis is that the drug has no effect, thus the reduction in FVC should be zero. Let’s first find the probability of observing an FVC reduction of greater than <span class="math inline">\(137\)</span> given the null hypothesis.</p>
<p>We calculate the standard error based on the <span class="math inline">\(14\)</span> participants as <span class="math inline">\(\sigma_{\bar{X}} = \frac{223}{\sqrt{14}} = 60.\)</span> The z-score associated with the mean reduction in FVC is given by <span class="math inline">\(\frac{(137 - 0)}{60} = 2.28\)</span>. The probability of observing a value further from the mean by at least <span class="math inline">\(2.28\)</span> standard deviations, which is also the p-value, which is <span class="math inline">\(2.2\%\)</span> (or <span class="math inline">\(0.022\)</span>).</p>
<p>This is a small probability that the drug is having an effect just by chance. Why did we obtain a small probability? Because it’s effect by chance should be zero. But because we’re working with a sample (<span class="math inline">\(N=14\)</span>) that is randomly drawn from the population, the observed mean reduction will fluctuate from sample to sample. Based on data from our sample, the reduction was <span class="math inline">\(137\)</span>. But how large is <span class="math inline">\(137\)</span>? We don’t know without some form of calibration, which can be obtained by the information that was given to us: <span class="math inline">\(\sigma_{\bar{X}}\)</span>.</p>
<p>From these data, we can believe that the drug helps prevent deterioration in lung function. At least the data are consistent with this notion in a probabilistic sense. Another way to think about this, it would be somewhat irrational to think that we could obtain the observed result just by chance. Maybe not entirely with a p of basically <span class="math inline">\(\frac{2}{100}\)</span> but probably with a p of <span class="math inline">\(\frac{1}{1000}\)</span>. But obviously this is somewhat subjective.</p>
<hr />
</div>
<div id="the-flaw-in-the-z-test" class="section level2">
<h2>The flaw in the z-test</h2>
<p>Is the above reasoning correct? To understand this we need to understand the difference between the first and second examples.</p>
<p>In our first example, an <strong>oracle</strong> provided us with the standard deviation of the population. Some all-knowing being told us what the population <span class="math inline">\(\sigma\)</span> was. But in the second example the standard error was based on the sample. To make the distinction clear, we call the standard error based on sample data <span class="math inline">\(s_{\bar{X}}\)</span>.</p>
<p><u>Important aside</u>: why use the sample and not the population? Populations are essentially Platonic objects. We typically don’t have complete knowledge about the objects we want to study. If we did we wouldn’t need to study them in the first place! So we have to draw samples and do the best based on finite amounts of data. Another way to think about this is that oracles don’t walk around waiting to be interrogated. Maybe they were around in ancient Greece, but not anymore…</p>
<p>Gossett (which published under the pseudonym Student) showed that when the standard error is estimated from sample data, the statistic <span class="math inline">\(\frac{\bar{x} - \mu}{\sigma_{\bar{X}}}\)</span> is not normal, but follows a <em>t</em>-distribution with <span class="math inline">\(N - 1\)</span> degrees of freedom (df). The <em>t</em>-distribution looks very much like a normal, but has what we call heavier tails, that is, more mass along the tails relative to the normal.</p>
<p>Thus, the probability associated with a <em>t</em>-score of <span class="math inline">\(2.28\)</span> with <span class="math inline">\(14 - 1\)</span> degrees of freedom can be calculated to be <span class="math inline">\(4\%\)</span> (or <span class="math inline">\(0.04\)</span>). The p-value obtained from the z-test (<span class="math inline">\(2.2\%\)</span>) overstates the evidence against the null hypothesis (this is consistent with the fact that a normal is thinner along the tails than the <em>t</em>-distribution).</p>
<hr />
<p>Examples are borrowed from <a href="http://myweb.uiowa.edu/pbreheny/4120/s18/index.html">Introduction to Biostatistics</a> kindly offered by Patrick Breheny at UIowa.</p>
<p>Post by Manasij Venkatesh, with edits by L. Pessoa.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
